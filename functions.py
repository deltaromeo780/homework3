import numpy as np


def set_initial_parameters(length):
    return (np.random.random(size=(length, 1)) - 0.5) * 10
# funkcja inicjuje poczÄ…tkowe wartoÅ›ci wag
# z przwedziaÅ‚u od 0 do 1, przenosi do -0.5; 0.5 ostateczny przedziaÅ‚ -5.5 ; 5.5


def linear_regression_hypothesis(X, w):
    return np.dot(X, w)  # iloczyn skalarny X(m,n) i w(n,1)
# m - liczba prÃ³bek n- liczba cech
# Celem funkcji linear_regression_hypothesis jest obliczenie przewidywanych wartoÅ›ci y
# dla danego zbioru cech X i wag w w modelu liniowej regresji.


def error_function(h, y):
    m = len(h)
    errors = h - y  # errors to wektor rÃ³Å¼nic miÄ™dzy przewidywanymi wartoÅ›ciami a rzeczywistymi wartoÅ›ciami.
    return np.dot(errors.T, errors) / (2 * m)


''' np.dot(errors.T, errors) oblicza iloczyn skalarny wektora errors (transponowanego) ze samym sobÄ…, co daje sumÄ™
 kwadratÃ³w bÅ‚Ä™dÃ³w. Dzielimy tÄ™ sumÄ™ przez 2 * m, gdzie 2 jest czynnikiem normalizacyjnym dla wygody i zgodnoÅ›ci
  z konwencjÄ… uÅ¼ywanÄ… w regresji liniowej.'''


def error_gradient(h, y, X):
    m = len(y)  # liczba prÃ³bek
    gradient = np.dot(X.T, (h - y)) / m
    return gradient


''' h: wektor przewidywanych wartoÅ›ci (hipoteza).
y: wektor rzeczywistych wartoÅ›ci (etykiety).
X: macierz cech, gdzie kaÅ¼dy wiersz reprezentuje jednÄ… prÃ³bkÄ™, a kaÅ¼da kolumna reprezentuje jednÄ… cechÄ™.'''


def gradient_descent_step(w, gradient, alpha):
    return w - alpha * gradient  # aktualizacja wag

# w: aktualne wartoÅ›ci wag modelu.
# gradient: gradient funkcji kosztu w punkcie w.
# alpha: wspÃ³Å‚czynnik uczenia, kontrolujÄ…cy wielkoÅ›Ä‡ kroku aktualizacji wag.


def solve_optimization_task(w_init, X, y, alpha0=0.1, epsilon=1e-6):
    # w_init: poczÄ…tkowe wartoÅ›ci wspÃ³Å‚czynnikÃ³w regresji.
    # X: macierz cech.
    # y: wektor wynikÃ³w.
    # alpha0: poczÄ…tkowy krok uczenia.
    # epsilon: prÃ³g zakoÅ„czenia algorytmu.
    h_init = linear_regression_hypothesis(X, w_init)
    gradient_init = error_gradient(h_init, y, X)

    gradient = gradient_init
    w_prev = w_init
    h_prev = h_init
    alpha = alpha0

    error_prev = error_function(h_prev, y)

    steps = 0

    # h_init oblicza poczÄ…tkowÄ… hipotezÄ™ regresji liniowej.
    # gradient_init oblicza poczÄ…tkowy gradient bÅ‚Ä™du.
    # gradient, w_prev, h_prev oraz alpha sÄ… inicjalizowane odpowiednio.
    # error_prev oblicza poczÄ…tkowÄ… wartoÅ›Ä‡ funkcji bÅ‚Ä™du.
    # steps licznik krokÃ³witeracji.

    while np.sqrt(np.dot(gradient.T, gradient)) > epsilon:
        steps += 1
        w = gradient_descent_step(w_prev, gradient, alpha)
        h = linear_regression_hypothesis(X, w)
        error = error_function(h, y)
        if error > error_prev:
            alpha = alpha / 2
        else:
            h_prev = h
            w_prev = w
            error_prev = error
            gradient = error_gradient(h, y, X)

        if steps > 5000:
            print("Optimal solution hasn't been found")
            break

        '''PÄ™tla wykonuje siÄ™ dopÃ³ki norma gradientu jest wiÄ™ksza niÅ¼ epsilon.
        Aktualizuje licznik krokÃ³w.
        w jest aktualizowane przez wykonanie kroku gradientu.
        h oblicza nowÄ… hipotezÄ™.
        error oblicza nowÄ… wartoÅ›Ä‡ bÅ‚Ä™du.
        JeÅ›li nowy bÅ‚Ä…d jest wiÄ™kszy od poprzedniego, krok uczenia alpha jest zmniejszany.
        JeÅ›li nowy bÅ‚Ä…d jest mniejszy lub rÃ³wny, aktualizowane sÄ… h_prev, w_prev, error_prev oraz gradient.
        JeÅ›li liczba krokÃ³w przekracza 5000, algorytm przerywa i informuje, Å¼e rozwiÄ…zanie nie zostaÅ‚o znalezione.'''

    print(
        f"Optimization task has been ended after {steps} steps, alpha = {alpha}, epsilon = {epsilon}, "
        f"\nObjective function value = {error}"
    )

    return w

    # Po zakoÅ„czeniu pÄ™tli drukuje informacje o liczbie krokÃ³w, ostatniej wartoÅ›ci alpha, wartoÅ›ci epsilon oraz
    # koÅ„cowej wartoÅ›ci funkcji celu. Funkcja zwraca koÅ„cowe wspÃ³Å‚czynniki regresji w.


def solve_linear_equations_system(X, y):
    w = np.dot(np.dot(np.linalg.inv((np.dot(X.T, X))), X.T), y)
    return w


''' Funkcja wykorzystuje wzÃ³r analityczny do obliczenia wspÃ³Å‚czynnikÃ³w regresji liniowej ğ‘¤
 za pomocÄ… pseudoodwrotnoÅ›ci macierzy:
ğ‘¤ = (ğ‘‹ğ‘‡ * ğ‘‹)âˆ’1 * ğ‘‹ğ‘‡ğ‘¦ 
np.dot(X.T, X) oblicza iloczyn macierzy transponowanej X i X.
np.linalg.inv(...) oblicza odwrotnoÅ›Ä‡ powyÅ¼szej macierzy.
np.dot(..., X.T) oblicza iloczyn odwrotnoÅ›ci z macierzÄ… transponowanÄ… X.
np.dot(..., y) oblicza ostateczny iloczyn z wektorem wynikÃ³w y, dajÄ…c wspÃ³Å‚czynniki w.
Funkcja zwraca obliczone wspÃ³Å‚czynniki w.'''


def compare_solutions(lin_sol, opt_sol):
    # lin_sol: rozwiÄ…zanie uzyskane metodÄ… rozwiÄ…zywania ukÅ‚adu rÃ³wnaÅ„ liniowych.
    # opt_sol: rozwiÄ…zanie uzyskane metodÄ… optymalizacji.
    print(f"Solution of linear equations system, \nw_lin_sol = \n{lin_sol}")
    print(f"Solution of optimization task, \nw_opt_sol = \n{opt_sol}")

    np.testing.assert_array_almost_equal(lin_sol, opt_sol, decimal=0.001)

# Funkcja uÅ¼ywa np.testing.assert_array_almost_equal do porÃ³wnania, czy oba rozwiÄ…zania sÄ… prawie rÃ³wne,
# z dokÅ‚adnoÅ›ciÄ… do 0.001.
# JeÅ›li rozwiÄ…zania rÃ³Å¼niÄ… siÄ™ bardziej niÅ¼ o 0.001, funkcja zgÅ‚asza bÅ‚Ä…d
